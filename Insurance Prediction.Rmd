---
title: "Medical Cost Prediction"
author: "Matthew Buddensick"
date: "9/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
pacman::p_load(tidyverse, tidymodels, naniar, janitor, ggcorrplot)
theme_set(theme_classic())
```

## Data

In this file we will be working on predicting medical insurance costs. The dataset can be found on [kaggle](https://www.kaggle.com/mirichoi0218/insurance) and includes 1339 rows and 7 columns. We will be using the tidymodels package in order to create our models.

```{r}
insurance <- read_csv("insurance.csv")
```

```{r}
summary(insurance)
```

```{r}
miss_var_summary(insurance)
```

There is no missing data in the data set, so we can now manipulate our data to create factor variables. We will also make amount of children a factor variable because there is a set amount of values in the dataset.

```{r}
insurance <- insurance %>% 
    mutate_if(is.character, factor) %>% 
    mutate(children = factor(children, levels = c(0, 1, 2, 3, 4 ,5), ordered = TRUE))
```

```{r}
summary(insurance)
```

## EDA

### Numeric Variables
### Correlation Plot

```{r}
insurance_cor <- insurance %>% 
    select(where(is.numeric)) %>% 
    cor()
ggcorrplot(insurance_cor, method = "square", type = "upper", colors = c("blue", "white", "red")) +
    labs(title = "Correlations")
```

We can see from the above graph that insurance costs seem to be correlated with age and bmi. It also looks like there is a very small correlation between bmi and age.

### Age

```{r}
ggplot(insurance, aes(x = age, y = charges)) +
    geom_point() +
    labs(title = "Charges vs Age", x = "Age", y = "Charges")
```

As age increases it seems like insurance costs also rise. This makes sense as the elderly are at higher risks of health related problems.

### BMI

```{r message=FALSE, warning=FALSE}
ggplot(insurance, aes(x = bmi, y = charges)) +
    geom_point() +
    labs(title = "Charges vs Age", x = "BMI", y = "Charges") +
    geom_smooth(method = "lm")
```

An **outlier** is a point whose **y** values does not follow the trend of the rest of the data. A point has **high leverage** if there are extreme **x** values in the data.

Here we can see that it looks like there are two clusters of points, but there are not really any points that have high leverage. What happens if we add a new point to the data to add a point that has high leverage, and also a point that is clearly an outlier.

```{r include=FALSE}
insurance_high_leverage <- insurance %>% 
    add_row(bmi = 80, charges = 1000)
high_leverage_point <- insurance_high_leverage %>% 
    filter(bmi > 70)
```

```{r include=FALSE}
insurance_high_y <- insurance %>% 
    add_row(bmi = 25, charges = 100000)
outlier_point <-insurance_high_y %>% 
    filter(charges > 80000)
```

```{r message=FALSE, warning=FALSE}
ggplot(insurance_high_leverage, aes(x = bmi, y = charges)) +
    geom_point() +
    geom_point(data = high_leverage_point, aes(x = bmi, y = charges, color = "red"), size = 5) +
    labs(title = "Graph with High Leverage Point", x = "BMI", y = "Charges", 
         caption = "The point in red has been articifally added to the data") +
    geom_smooth(method = "lm") +
    theme(legend.position = "none")
```

```{r message=FALSE, warning=FALSE}
ggplot(insurance_high_y, aes(x = bmi, y = charges)) +
    geom_point() +
    geom_point(data = outlier_point, aes(x = bmi, y = charges, color = "red"), size = 5) +
    labs(title = "Graph with Extreme Outlier", x = "BMI", y = "Charges",
         caption = "The point in red has been articifally added to the data") +
    geom_smooth(method = "lm") +
    theme(legend.position = "none")
```

We can see why outliers and high leverage points might be influential to our best fit line. We need to be careful when looking at these extreme points and determine which points may be highly influential. These extreme values can affect measures we use to evaluate our models, such as R^2^. To read more about outliers and high leverage points here is a [lesson from Penn State](https://online.stat.psu.edu/stat462/node/170/#:~:text=In%20short%3A,is%20particularly%20high%20or%20low.)

### Nominal Variables
### Sex

```{r}
ggplot(insurance, aes(x = sex, y = charges, fill = sex)) +
    geom_boxplot() +
    labs(title = "Charges vs Sex", y = "Charges", x = "Sex") +
    theme(legend.position = "none")
```

The median costs for males seems to be about the same as females. The IQR seems to span a larger range for males than females, and the maximum costs for both sexes is about $60,000.

### Number of Children

```{r}
ggplot(insurance, aes(x = children, y = charges, fill = children)) +
    geom_boxplot() +
    labs(title = "Number of Children vs Charges", y = "Charges", x = "Number of Children") +
    theme(legend.position = "none")
```

```{r}
ggplot(insurance, aes(x = children, fill = children)) +
    geom_bar() +
    labs(title = "Total of Children", y = "Count", x = "Children") +
    theme(legend.position = "none")
```

The median charge for children seems to be about the same for all levels. We can also see that there are a lot more people with only a couple children than have 4 or 5 children. Because of this we will lump the levels together so that we just have 4 levels instead of 6. These levels will be 0, 1, 2, and other.

```{r}
insurance <- insurance %>% 
    mutate(children = fct_lump(factor(children, n = 4)))
head(insurance$children)
```

Here we can see that the levels have Other being greater than 0, 1 and 2. This is correct since other includes people that have 3, 4 and 5 children. Lets see if the class balance is any better after making this change.

```{r}
ggplot(insurance, aes(x = children, fill = children)) +
    geom_bar() +
    labs(title = "Total of Children", y = "Count", x = "Children") +
    theme(legend.position = "none")
```

There are still a lot more people that have no children, but we no longer have any levels that appear only a couple of times in the data set.

### Smoker

Before even looking at the differences in insurance costs for smokers vs non-smokers, we can assume that smokers will have higher insurance costs due to the likelihood of having more medical conditions.

```{r}
ggplot(insurance, aes(x = smoker, y = charges, fill = smoker)) +
    geom_boxplot() +
    labs(title = "Charges vs Smoker", y = "Charges", x = "Smoker") +
    theme(legend.position = "none")
```

We can see a massive difference in the costs of insurance between people who smoke and people who don't. It looks like the median costs is over 3x more for someone who smokes.

### Region

```{r}
ggplot(insurance, aes(x = region, y = charges, fill = region)) +
    geom_boxplot() +
    labs(title = "Charges vs Region", y = "Charges", x = "Region") +
    theme(legend.position = "none")
```

Median costs seem to be relatively even for each region. If anything it looks like the northeast region might have slightly higher costs than the rest.

### The Dependent Variable, Charges

```{r}
ggplot(insurance, aes(x = charges)) +
    geom_histogram(fill = "#00BFC4") +
    labs(title = "Histogram of Charges", y = "Count", x = "Charges")
```

Here we can see that the freqeuncy distribution of our dependent variable is skewed to the right, and that a majority of people have insurance costs of under $10,000.

## Modeling

```{r}
set.seed(42)
insurance_split <- initial_split(insurance)
insurance_training <- training(insurance_split)
insurance_testing <- testing(insurance_split)
```

```{r}
set.seed(42)
kfolds <- vfold_cv(insurance_training, 10)
```

```{r}
insurance_recipe <- recipe(charges ~., data = insurance_training) %>%  
    step_dummy(all_nominal())
```

```{r}
insurance_wf <- workflow() %>% 
    add_recipe(insurance_recipe)
```

```{r}
metrics_used <- metric_set(rmse, rsq, mae)
```

### Linear Regression

```{r}
linear_spec <- linear_reg() %>% 
    set_mode("regression") %>% 
    set_engine("lm")
```

```{r}
linear_model <- insurance_wf %>% 
    add_model(linear_spec) %>% 
    fit_resamples(
        resamples = kfolds,
        metrics = metrics_used,
        control = control_resamples(save_pred = TRUE)
    )
```

```{r}
collect_metrics(linear_model)
```

**MAE** measures the average magnitude of the errors in our predictions using absolute difference between the prediction and actual observation, so we could say on average on model is off by about $6133. **RMSE** measures the average of squared differences between our predictions and actual observations. The lower these values are the better our model is. **RSQ** is our R^2^ value (between 0 and 1), and is the variation that is explained by the model and a higher value is better.

```{r}
linear_final <- insurance_wf %>% 
    add_model(linear_spec) %>% 
    last_fit(insurance_split, metrics = metrics_used)
```

```{r}
(linear_results <- collect_metrics(linear_final)) %>% 
    cbind(Model = "Linear")
```

Our model has an rmse of about 5971 and an R^2^ value of about .73 on the testing data. We can say this model fits the data okay for now, since the model only performed slightly worse on the testing data than the training data. But we can try other models to see if we can get a model that fits the data better.

## Tuned Random Forest

```{r}
forest_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>% 
    set_mode("regression") %>% 
    set_engine("randomForest")
```

```{r}
forest_wf <- workflow() %>% 
    add_recipe(insurance_recipe) %>% 
    add_model(forest_spec)
```


```{r}
doParallel::registerDoParallel()
forest_tuned <- tune_grid(forest_wf, resamples = kfolds, grid  = 5)
```

```{r}
forest_model <- forest_tuned %>% 
    select_best("rmse")
forest_model
```

```{r}
forest_final_params <- finalize_model(forest_spec, forest_model)
forest_final_params
```

```{r}
forest_final_wf <- workflow() %>% 
    add_recipe(insurance_recipe) %>% 
    add_model(forest_final_params)
```

```{r}
(forest_results <- forest_final_wf %>% last_fit(insurance_split, metrics = metrics_used) %>% collect_metrics()) %>% 
    cbind(Model = "Random Forest")
```

### Tuned KNN

```{r}
knn_spec <- nearest_neighbor(neighbors = tune(), weight_func = tune(), dist_power = tune()) %>% 
    set_mode("regression") %>% 
    set_engine("kknn")
```

```{r}
knn_wf <- workflow() %>% 
    add_recipe(insurance_recipe) %>% 
    add_model(knn_spec)
```

```{r message=FALSE, warning=FALSE}
knn_tuned <- tune_grid(knn_wf, resamples = kfolds, grid = 5)
```

```{r}
knn_model <- knn_tuned %>% 
    select_best("rmse")
knn_model
```

```{r}
knn_final_params <- finalize_model(knn_spec, knn_model)
knn_final_params
```

```{r}
knn_final_wf <- workflow() %>%
    add_recipe(insurance_recipe) %>% 
    add_model(knn_final_params)
```

```{r}
(knn_results <- knn_final_wf %>% last_fit(insurance_split, metrics = metrics_used) %>% collect_metrics()) %>% 
    cbind(Model = "KNN")
```

## Evaluate Models

```{r}
compare_results <- rbind(linear_results, forest_results, knn_results)
(compare_results <- cbind(compare_results, 
                        Model = c("Linear", "Linear", "Linear", "Random Forest", "Random Forest", "Random Forest",
                                  "KNN", "KNN", "KNN")))
```

```{r}
ggplot(compare_results, aes(x = .metric, y = .estimate, fill = Model)) +
    geom_col(position = "dodge") +
    facet_wrap(~ .metric, scales = "free", strip.position = "bottom") +
    labs(title = "Model Metrics", y = "Estimate", x = "Metric") +
    theme(axis.text.x.bottom = element_blank(),
          axis.ticks = element_blank())
```

The random forest model has the lowest RMSE value and also the highest R^2^ value. Therefore the random forest model performed the best out of all three models. We can also see that the linear model performed the worst with the highest RMSE value and lowest R^2^.